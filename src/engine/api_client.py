"""
APIå®¢æˆ·ç«¯æ¨¡å—ï¼Œè´Ÿè´£ä¸æ¨¡å‹APIé€šä¿¡
"""
import time
import json
import asyncio
import aiohttp
from typing import Dict, Any, Optional, AsyncGenerator
from src.utils.logger import setup_logger
from src.utils.token_counter import token_counter  # å¯¼å…¥tokenè®¡æ•°å™¨
from src.utils.config import config

logger = setup_logger("api_client")


def _diagnose_disconnect_error(error: Exception, data_received_count: int, time_since_last_data: float, prompt_len: int) -> str:
    """
    è¯Šæ–­è¿æ¥æ–­å¼€é”™è¯¯çš„åŸå› 
    
    Args:
        error: å¼‚å¸¸å¯¹è±¡
        data_received_count: å·²æ¥æ”¶çš„æ•°æ®å—æ•°é‡
        time_since_last_data: è·ç¦»æœ€åä¸€æ¬¡æ”¶åˆ°æ•°æ®çš„æ—¶é—´ï¼ˆç§’ï¼‰
        prompt_len: è¯·æ±‚prompté•¿åº¦
    
    Returns:
        è¯Šæ–­ä¿¡æ¯å­—ç¬¦ä¸²
    """
    diagnosis = []
    
    if isinstance(error, aiohttp.ServerDisconnectedError):
        diagnosis.append("ğŸ”´ æœåŠ¡å™¨ä¸»åŠ¨æ–­å¼€è¿æ¥")
        if data_received_count == 0:
            diagnosis.append("  - æœªæ”¶åˆ°ä»»ä½•æ•°æ®ï¼Œè¿æ¥åœ¨å“åº”å¼€å§‹å‰æ–­å¼€")
            diagnosis.append("  - å¯èƒ½åŸå› ï¼šæœåŠ¡å™¨ç«¯è¶…æ—¶ã€èµ„æºé™åˆ¶ã€è¯·æ±‚æ ¼å¼é”™è¯¯")
        else:
            diagnosis.append(f"  - å·²æ¥æ”¶ {data_received_count} ä¸ªæ•°æ®å—åæ–­å¼€")
            if time_since_last_data < 5:
                diagnosis.append("  - âš ï¸ åœ¨æ­£å¸¸æ¥æ”¶æ•°æ®æ—¶çªç„¶æ–­å¼€ï¼Œå¯èƒ½æ˜¯æ•°æ®æ¥æ”¶é€Ÿåº¦é—®é¢˜")
            diagnosis.append("  - å¯èƒ½åŸå› ï¼šæœåŠ¡å™¨å¤„ç†è¶…æ—¶ã€èµ„æºè€—å°½ã€æ•°æ®æ¥æ”¶ä¸åŠæ—¶")
        
        if prompt_len > 1000:
            diagnosis.append(f"  - è¯·æ±‚è¾ƒé•¿ï¼ˆ{prompt_len}å­—ç¬¦ï¼‰ï¼Œå¯èƒ½è¶…å‡ºæœåŠ¡å™¨å¤„ç†èƒ½åŠ›")
    elif isinstance(error, asyncio.TimeoutError):
        diagnosis.append("â±ï¸ å®¢æˆ·ç«¯è¶…æ—¶")
        if data_received_count == 0:
            diagnosis.append("  - æœªæ”¶åˆ°ä»»ä½•æ•°æ®")
            diagnosis.append("  - å¯èƒ½åŸå› ï¼šæœåŠ¡å™¨å“åº”æ…¢ã€ç½‘ç»œå»¶è¿Ÿé«˜ã€è¿æ¥å»ºç«‹å¤±è´¥")
        else:
            diagnosis.append(f"  - åœ¨æ¥æ”¶ {data_received_count} ä¸ªæ•°æ®å—åè¶…æ—¶")
            diagnosis.append(f"  - è·ç¦»æœ€åä¸€æ¬¡æ•°æ®å·²è¿‡å» {time_since_last_data:.2f} ç§’")
            diagnosis.append("  - å¯èƒ½åŸå› ï¼šæœåŠ¡å™¨ç”Ÿæˆé€Ÿåº¦æ…¢ã€ç½‘ç»œä¸ç¨³å®š")
    else:
        diagnosis.append("ğŸ”Œ å®¢æˆ·ç«¯è¿æ¥é”™è¯¯")
        diagnosis.append(f"  - é”™è¯¯ç±»å‹: {type(error).__name__}")
        diagnosis.append("  - å¯èƒ½åŸå› ï¼šç½‘ç»œé—®é¢˜ã€DNSè§£æå¤±è´¥ã€é˜²ç«å¢™é™åˆ¶")
    
    return "\n".join(diagnosis)

class StreamStats:
    """æµå¼è¾“å‡ºç»Ÿè®¡"""
    def __init__(self, model_name: str = None):
        self.total_chars = 0
        self.total_tokens = 0
        self.total_time = 0.0
        self.last_update_time = time.time()
        self.current_char_speed = 0.0
        self.current_token_speed = 0.0
        self.char_speeds = []
        self.token_speeds = []
        self.model_name = model_name
    
    def update(self, new_text: str):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        time_diff = current_time - self.last_update_time
        
        if time_diff > 0:
            # è®¡ç®—æ–°å¢å­—ç¬¦æ•°
            new_chars = len(new_text)
            # ä½¿ç”¨tiktokenè®¡ç®—tokenæ•°
            new_tokens = token_counter.count_tokens(new_text, self.model_name)
            
            # è®¡ç®—å­—ç¬¦é€Ÿåº¦
            self.current_char_speed = new_chars / time_diff
            self.char_speeds.append(self.current_char_speed)
            
            # è®¡ç®—tokené€Ÿåº¦
            self.current_token_speed = new_tokens / time_diff
            self.token_speeds.append(self.current_token_speed)
            
            # æ›´æ–°æ€»è®¡æ•°
            self.total_chars += new_chars
            self.total_tokens += new_tokens
            self.total_time += time_diff
        
        self.last_update_time = current_time
    
    @property
    def avg_char_speed(self) -> float:
        """å¹³å‡å­—ç¬¦ç”Ÿæˆé€Ÿåº¦ï¼ˆå­—ç¬¦/ç§’ï¼‰"""
        if self.total_time > 0:
            return self.total_chars / self.total_time
        return 0.0
    
    @property
    def avg_token_speed(self) -> float:
        """å¹³å‡tokenç”Ÿæˆé€Ÿåº¦ï¼ˆtoken/ç§’ï¼‰"""
        if self.total_time > 0:
            return self.total_tokens / self.total_time
        return 0.0

class APIResponse:
    """APIå“åº”æ•°æ®ç±»"""
    def __init__(
        self,
        success: bool,
        response_text: str = "",
        error_msg: str = "",
        input_tokens: int = 0,
        output_tokens: int = 0,
        duration: float = 0.0,
        start_time: float = 0.0,
        end_time: float = 0.0,
        model_name: str = "",
        stream_stats: Optional[StreamStats] = None,
        first_token_latency: Optional[float] = None
    ):
        self.success = success
        self.response_text = response_text
        self.error_msg = error_msg
        self.input_tokens = input_tokens
        self.output_tokens = output_tokens
        # å…¼å®¹æ—§å­—æ®µå‘½å
        self.tokens_generated = output_tokens
        self.duration = duration
        self.start_time = start_time
        self.end_time = end_time
        self.model_name = model_name
        self.stream_stats = stream_stats
        # é¦–å­—èŠ‚/é¦–tokenå»¶è¿Ÿï¼ˆç§’ï¼‰
        self.first_token_latency = first_token_latency
    
    @property
    def generation_speed(self) -> float:
        """è®¡ç®—ç”Ÿæˆé€Ÿåº¦ï¼ˆå­—ç¬¦/ç§’ï¼‰"""
        if self.stream_stats:
            # ä½¿ç”¨æµå¼ç»Ÿè®¡çš„å¹³å‡é€Ÿåº¦
            return self.stream_stats.avg_char_speed
        elif self.duration > 0 and self.response_text:
            # å¦‚æœæ²¡æœ‰æµå¼ç»Ÿè®¡ï¼Œä½¿ç”¨æ€»å­—ç¬¦æ•°é™¤ä»¥æ€»æ—¶é—´
            return len(self.response_text) / self.duration
        return 0.0
    
    @property
    def total_chars(self) -> int:
        """è·å–æ€»å­—ç¬¦æ•°"""
        return len(self.response_text) if self.response_text else 0
    
    @property
    def total_tokens(self) -> int:
        """è·å–æ€»tokenæ•°"""
        return self.output_tokens

class APIClient:
    """APIå®¢æˆ·ç«¯ç±»"""
    def __init__(
        self,
        api_url: str,
        api_key: str,
        model: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        top_p: float = 0.9,
        timeout: int = 10,  # æ·»åŠ è¶…æ—¶å‚æ•°
        retry_count: int = 1,  # æ·»åŠ é‡è¯•æ¬¡æ•°å‚æ•°
        chat_path: str = "/chat/completions",
        extra_headers: Optional[Dict[str, str]] = None,
        extra_body_params: Optional[Dict[str, Any]] = None,
        stream: Optional[bool] = None
    ):
        # ç¡®ä¿ API URL æ ¼å¼æ­£ç¡®
        self.api_url = api_url.rstrip("/")
        if not self.api_url.endswith("/v1"):
            self.api_url += "/v1"
        self.api_key = api_key
        self.model = model
        self.chat_path = chat_path
        
        # ä½¿ç”¨ä¼ å…¥çš„è¶…æ—¶å’Œé‡è¯•è®¾ç½®
        self.connect_timeout = timeout
        self.max_retries = retry_count
        
        # å…¶ä»–å‚æ•°
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_p = top_p
        
        # æ¨¡å‹å‚æ•°
        self.model_params = {
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p
        }
        if extra_body_params:
            self.model_params.update(extra_body_params)
        # å…è®¸è¦†ç›–æµæ¨¡å¼
        self._force_stream = stream
        # é™„åŠ è¯·æ±‚å¤´
        self.extra_headers = extra_headers or {}
        
        # åˆ›å»ºå¼‚æ­¥HTTPä¼šè¯ï¼Œç¦ç”¨è¿æ¥å¹¶å‘é™åˆ¶
        default_headers = {"Authorization": f"Bearer {api_key}"} if api_key else {}
        default_headers.update(self.extra_headers)
        # æ”¹è¿›è¿æ¥å™¨é…ç½®ï¼Œå¢åŠ è¿æ¥æ± å¤§å°å’Œkeepaliveæ—¶é—´
        # å¯¹äºå‹åŠ›æµ‹è¯•ï¼Œå¢åŠ è¿æ¥æ± å¤§å°å’Œkeepaliveæ—¶é—´ï¼Œæé«˜è¿æ¥ç¨³å®šæ€§
        # å¯¹äºé•¿è¯·æ±‚å’Œæµå¼å“åº”ï¼Œéœ€è¦æ›´é•¿çš„keepaliveè¶…æ—¶æ—¶é—´ï¼ˆè‡³å°‘è¦†ç›–max_tokensç”Ÿæˆæ—¶é—´ï¼‰
        # å‡è®¾å¹³å‡ç”Ÿæˆé€Ÿåº¦20 tokens/sï¼Œ2048 tokenséœ€è¦çº¦20ç§’ï¼ŒåŠ ä¸Šç½‘ç»œå»¶è¿Ÿï¼Œè®¾ç½®300ç§’æ›´å®‰å…¨
        keepalive_timeout = max(300, (self.max_tokens // 20) * 2) if self.max_tokens else 300
        connector = aiohttp.TCPConnector(
            limit=0,  # æ— é™åˆ¶æ€»è¿æ¥æ•°
            limit_per_host=0,  # æ— é™åˆ¶æ¯ä¸ªä¸»æœºè¿æ¥æ•°
            keepalive_timeout=keepalive_timeout,  # æ ¹æ®max_tokensåŠ¨æ€è°ƒæ•´keepaliveè¶…æ—¶æ—¶é—´
            enable_cleanup_closed=True,  # å¯ç”¨æ¸…ç†å·²å…³é—­çš„è¿æ¥
            force_close=False,  # ä¸å¼ºåˆ¶å…³é—­è¿æ¥ï¼Œå…è®¸é‡ç”¨
            ttl_dns_cache=300,  # DNSç¼“å­˜TTL
            use_dns_cache=True,  # å¯ç”¨DNSç¼“å­˜
        )
        self.session = aiohttp.ClientSession(headers=default_headers, connector=connector)
        logger.info(f"åˆå§‹åŒ– API å®¢æˆ·ç«¯: URL={api_url}, model={model}, connect_timeout={self.connect_timeout}, max_retries={self.max_retries}")
    
    async def _recreate_session(self):
        """é‡æ–°åˆ›å»ºHTTPä¼šè¯"""
        if self.session and not self.session.closed:
            await self.session.close()
        default_headers = {"Authorization": f"Bearer {self.api_key}"} if self.api_key else {}
        default_headers.update(self.extra_headers)
        # ä½¿ç”¨ä¸åˆå§‹åŒ–æ—¶ç›¸åŒçš„keepaliveè¶…æ—¶é€»è¾‘
        keepalive_timeout = max(300, (self.max_tokens // 20) * 2) if self.max_tokens else 300
        connector = aiohttp.TCPConnector(
            limit=0,
            limit_per_host=0,
            keepalive_timeout=keepalive_timeout,
            enable_cleanup_closed=True,
            force_close=False,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        self.session = aiohttp.ClientSession(headers=default_headers, connector=connector)
        logger.info("å·²é‡æ–°åˆ›å»ºAPIå®¢æˆ·ç«¯ä¼šè¯")
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯ä¼šè¯"""
        if self.session and not self.session.closed:
            await self.session.close()
            logger.info("APIå®¢æˆ·ç«¯ä¼šè¯å·²å…³é—­")
    
    def _prepare_request(self, prompt: str) -> dict:
        """å‡†å¤‡è¯·æ±‚æ•°æ®"""
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆå…è®¸å¤–éƒ¨å¼ºåˆ¶è¦†ç›–ï¼‰
        use_stream = self._force_stream
        if use_stream is None:
            use_stream = config.get('openai_api.stream_mode', True)
        
        return {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "stream": use_stream,  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            **self.model_params  # åªåŒ…å«æ”¯æŒçš„å‚æ•°
        }
    
    async def _process_stream(
        self,
        response: aiohttp.ClientResponse
    ) -> AsyncGenerator[str, None]:
        """å¤„ç†æµå¼å“åº”"""
        last_data_time = time.time()  # è®°å½•æœ€åä¸€æ¬¡æ”¶åˆ°æ•°æ®çš„æ—¶é—´
        data_received_count = 0  # è®°å½•å·²æ¥æ”¶çš„æ•°æ®å—æ•°é‡
        try:
            async for line in response.content:
                # æ›´æ–°æœ€åæ¥æ”¶æ•°æ®çš„æ—¶é—´
                last_data_time = time.time()
                data_received_count += 1
                
                # æ£€æŸ¥è¿æ¥çŠ¶æ€ - åœ¨è¯»å–æ•°æ®å‰æ£€æŸ¥ï¼Œé¿å…åœ¨å·²å…³é—­çš„è¿æ¥ä¸Šæ“ä½œ
                may_continue = True
                if response.closed:
                    # logger.warning("æ£€æµ‹åˆ°å“åº”è¿æ¥å·²å…³é—­ï¼ˆåœ¨è¯»å–æ•°æ®æ—¶ï¼‰")
                    # å¦‚æœå·²ç»æ”¶åˆ°ä¸€äº›æ•°æ®ï¼Œå¯èƒ½æ˜¯æ­£å¸¸ç»“æŸï¼›å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨ä¸»åŠ¨æ–­å¼€
                    if data_received_count == 0:
                        raise aiohttp.ServerDisconnectedError("æœåŠ¡å™¨åœ¨å“åº”å¼€å§‹å‰æ–­å¼€è¿æ¥")
                    else:
                        # logger.info(f"è¿æ¥å·²å…³é—­ï¼Œä½†å·²æ”¶åˆ° {data_received_count} ä¸ªæ•°æ®å—ï¼Œå¯èƒ½æ˜¯æ­£å¸¸ç»“æŸ")
                        may_continue = False
                if may_continue:
                    line = line.decode('utf-8').strip()
                    if not line:  # ç©ºè¡Œï¼Œè·³è¿‡
                        continue
                    if line.startswith('data: '):
                        try:
                            data = json.loads(line[6:])
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
                            if data.get('choices') and data['choices'][0].get('finish_reason'):
                                logger.debug("æ”¶åˆ°æµå¼å“åº”ç»“æŸæ ‡è®°")
                                break
                            if data.get('choices'):
                                # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šdelta å’Œ text
                                content = (
                                    data['choices'][0].get('delta', {}).get('content', '') or
                                    data['choices'][0].get('text', '')
                                )
                                if content:
                                    yield content
                        except json.JSONDecodeError as e:
                            logger.debug(f"JSONè§£æå¤±è´¥ï¼Œè·³è¿‡è¯¥è¡Œ: {line[:100]}")
                            continue
                    elif line.startswith(':'):  # SSEæ³¨é‡Šè¡Œï¼Œè·³è¿‡
                        continue
                    else:
                        logger.debug(f"æœªè¯†åˆ«çš„SSEæ ¼å¼è¡Œ: {line[:100]}")
                except UnicodeDecodeError as e:
                    logger.warning(f"æµå¼å“åº”è§£ç é”™è¯¯: {e}ï¼Œè·³è¿‡è¯¥è¡Œ")
                    continue
        except asyncio.TimeoutError as e:
            # å®¢æˆ·ç«¯è¶…æ—¶ï¼šé•¿æ—¶é—´æ²¡æœ‰æ”¶åˆ°æ•°æ®
            time_since_last_data = time.time() - last_data_time
            logger.error(f"æµå¼å“åº”è¯»å–è¶…æ—¶: è·ç¦»æœ€åä¸€æ¬¡æ”¶åˆ°æ•°æ®å·²è¿‡å» {time_since_last_data:.2f} ç§’")
            logger.error(f"å·²æ¥æ”¶æ•°æ®å—æ•°é‡: {data_received_count}")
            if data_received_count == 0:
                logger.error("å®¢æˆ·ç«¯è¶…æ—¶ï¼šæœªæ”¶åˆ°ä»»ä½•æ•°æ®ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨å“åº”æ…¢æˆ–ç½‘ç»œé—®é¢˜")
            else:
                logger.error("å®¢æˆ·ç«¯è¶…æ—¶ï¼šåœ¨æ¥æ”¶æ•°æ®è¿‡ç¨‹ä¸­è¶…æ—¶ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨ç”Ÿæˆé€Ÿåº¦æ…¢æˆ–ç½‘ç»œä¸ç¨³å®š")
            raise
        except (aiohttp.ServerDisconnectedError, aiohttp.ClientConnectionError, ConnectionError) as e:
            # è¿æ¥æ–­å¼€é”™è¯¯ï¼ŒåŒºåˆ†è¶…æ—¶å’Œä¸»åŠ¨æ–­å¼€
            error_type = type(e).__name__
            error_msg = str(e)
            time_since_last_data = time.time() - last_data_time
            
            # ä½¿ç”¨è¯Šæ–­å‡½æ•°ç”Ÿæˆè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            # æ³¨æ„ï¼šè¿™é‡Œæ— æ³•è·å–prompt_lenï¼Œæ‰€ä»¥ä¼ å…¥0
            diagnosis = _diagnose_disconnect_error(e, data_received_count, time_since_last_data, 0)
            logger.error(f"æµå¼å“åº”è¿æ¥æ–­å¼€: {error_type} - {error_msg}")
            logger.error("é”™è¯¯è¯Šæ–­:\n%s", diagnosis)
            
            raise  # å‘ä¸Šä¼ é€’å¼‚å¸¸ï¼Œè®©generateæ–¹æ³•å¤„ç†é‡è¯•
        except Exception as e:
            logger.error(f"æµå¼è¾“å‡ºå¤„ç†å¼‚å¸¸: {type(e).__name__} - {e}")
            logger.error(f"å·²æ¥æ”¶æ•°æ®å—æ•°é‡: {data_received_count}")
            raise  # å‘ä¸Šä¼ é€’å¼‚å¸¸ï¼Œè®©generateæ–¹æ³•å¤„ç†
    
    async def generate(self, prompt: str) -> APIResponse:
        """ç”Ÿæˆå“åº”"""
        # å°†æœ¬åœ°åˆ†è¯è€—æ—¶ä¹Ÿè®¡å…¥é¦– token å»¶è¿Ÿ
        start_time = time.time()
        prompt_tokens = token_counter.count_tokens(prompt, self.model)
        stream_stats = StreamStats(self.model)  # ä¼ å…¥æ¨¡å‹åç§°
        full_response = []
        first_token_latency = None
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        use_stream = self._force_stream
        if use_stream is None:
            use_stream = config.get('openai_api.stream_mode', True)
        
        for attempt in range(self.max_retries):
            try:
                request_url = f"{self.api_url}{self.chat_path}"
                request_data = {
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": use_stream,  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
                    **self.model_params  # ä½¿ç”¨model_paramsä»£æ›¿ç›´æ¥æŒ‡å®šå‚æ•°
                }
                # æ‰“å°å®Œæ•´è¯·æ±‚æ•°æ®ä»¥ä¾›è°ƒè¯•
                logger.info(f"å‘é€è¯·æ±‚: URL={request_url}, model={self.model}, stream={request_data.get('stream')}")
                # å®‰å…¨åœ°æ‰“å°è¯·æ±‚ä½“ï¼Œé¿å…æ‰“å°è¿‡é•¿çš„å†…å®¹
                request_data_log = request_data.copy()
                if 'messages' in request_data_log and request_data_log['messages']:
                    # é™åˆ¶contenté•¿åº¦ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                    content = request_data_log['messages'][0].get('content', '')
                    if len(content) > 200:
                        request_data_log['messages'][0]['content'] = content[:200] + f"... (å·²æˆªæ–­ï¼Œæ€»é•¿åº¦: {len(content)})"
                logger.info(f"è¯·æ±‚ä½“: {json.dumps(request_data_log, ensure_ascii=False)}")
                
                # å¦‚æœ connect_timeout ä¸º Noneï¼Œåˆ™å…³é—­æ‰€æœ‰è¶…æ—¶é™åˆ¶
                if self.connect_timeout is None:
                    timeout_config = None  # å®Œå…¨å…³é—­è¶…æ—¶
                else:
                    # å¯¹äºæµå¼å“åº”ï¼Œéœ€è¦æ›´é•¿çš„è¯»å–è¶…æ—¶æ—¶é—´
                    # æ ¹æ®max_tokensä¼°ç®—ï¼šå‡è®¾100 tokens/sï¼Œ2048 tokenséœ€è¦çº¦20ç§’ï¼ŒåŠ ä¸Šç¼“å†²è®¾ç½®ä¸º60ç§’
                    read_timeout = max(60, (self.max_tokens // 100) + 30) if self.max_tokens and use_stream else None
                    timeout_config = aiohttp.ClientTimeout(
                        connect=self.connect_timeout,
                        sock_connect=self.connect_timeout,
                        sock_read=read_timeout,  # æµå¼å“åº”æ—¶è®¾ç½®è¯»å–è¶…æ—¶
                        total=None  # ä¸é™åˆ¶æ€»ä½“è¶…æ—¶ï¼Œç”±sock_readæ§åˆ¶
                    )
                async with self.session.post(
                    request_url,  # å…è®¸è‡ªå®šä¹‰è·¯å¾„
                    json=request_data,
                    timeout=timeout_config
                ) as response:
                    if response.status == 200:
                        try:
                            # æ ¹æ®é…ç½®å†³å®šå¤„ç†æ–¹å¼
                            if use_stream:
                                # æµå¼è¾“å‡ºå¤„ç†
                                try:
                                    # æµå¼è¾“å‡ºå¤„ç†ï¼šåŠæ—¶æ¶ˆè´¹æ•°æ®ï¼Œé¿å…æ•°æ®ç§¯å‹
                                    # åœ¨å‹åŠ›æµ‹è¯•åœºæ™¯ä¸‹ï¼ŒåŠæ—¶å¤„ç†æ•°æ®éå¸¸é‡è¦ï¼Œé¿å…æœåŠ¡å™¨å› å®¢æˆ·ç«¯æ¥æ”¶æ…¢è€Œæ–­å¼€è¿æ¥
                                    async for chunk in self._process_stream(response):
                                        full_response.append(chunk)
                                        stream_stats.update(chunk)
                                        if first_token_latency is None:
                                            first_token_latency = time.time() - start_time
                                        # æ³¨æ„ï¼šå¼‚æ­¥è¿­ä»£å™¨ä¼šè‡ªåŠ¨è®©å‡ºæ§åˆ¶æƒï¼Œä¸éœ€è¦é¢å¤–çš„sleep
                                        # ä½†å¦‚æœå‹åŠ›æµ‹è¯•æ—¶ä»æœ‰é—®é¢˜ï¼Œå¯èƒ½æ˜¯äº‹ä»¶å¾ªç¯è¿‡è½½æˆ–ç½‘ç»œé—®é¢˜
                                except asyncio.TimeoutError as timeout_error:
                                    # å®¢æˆ·ç«¯è¶…æ—¶ï¼šé•¿æ—¶é—´æ²¡æœ‰æ”¶åˆ°æ•°æ®
                                    error_msg_str = str(timeout_error)
                                    logger.error(f"æµå¼å“åº”è¯»å–è¶…æ—¶: {error_msg_str}")
                                    if not full_response:
                                        # æ²¡æœ‰æ”¶åˆ°ä»»ä½•å“åº”ï¼ŒæŠ›å‡ºå¼‚å¸¸è®©é‡è¯•æœºåˆ¶å¤„ç†
                                        raise
                                    # æœ‰éƒ¨åˆ†å“åº”ï¼Œè¿”å›éƒ¨åˆ†ç»“æœä½†æ ‡è®°ä¸ºå¤±è´¥
                                    end_time = time.time()
                                    return APIResponse(
                                        success=False,
                                        response_text="".join(full_response),
                                        error_msg=f"å®¢æˆ·ç«¯è¯»å–è¶…æ—¶: {error_msg_str}",
                                        input_tokens=prompt_tokens,
                                        output_tokens=stream_stats.total_tokens,
                                        duration=end_time - start_time,
                                        start_time=start_time,
                                        end_time=end_time,
                                        model_name=self.model,
                                        stream_stats=stream_stats,
                                        first_token_latency=first_token_latency
                                    )
                                except (aiohttp.ServerDisconnectedError, aiohttp.ClientConnectionError, ConnectionError) as stream_error:
                                    # æµå¼å“åº”è¿‡ç¨‹ä¸­è¿æ¥æ–­å¼€ï¼ŒåŒºåˆ†æœåŠ¡å™¨ä¸»åŠ¨æ–­å¼€å’Œå®¢æˆ·ç«¯é”™è¯¯
                                    error_type = type(stream_error).__name__
                                    error_msg_str = str(stream_error)
                                    
                                    # åˆ¤æ–­æ˜¯æœåŠ¡å™¨ä¸»åŠ¨æ–­å¼€è¿˜æ˜¯å®¢æˆ·ç«¯é”™è¯¯
                                    is_server_disconnect = isinstance(stream_error, aiohttp.ServerDisconnectedError)
                                    
                                    # è®¡ç®—æ•°æ®æ¥æ”¶ç»Ÿè®¡
                                    data_chunks = len(full_response)
                                    total_chars = len("".join(full_response))
                                    time_elapsed = time.time() - start_time
                                    
                                    # ä½¿ç”¨è¯Šæ–­å‡½æ•°
                                    diagnosis = _diagnose_disconnect_error(stream_error, data_chunks, 0, len(prompt))
                                    logger.error(f"æµå¼å“åº”è¿‡ç¨‹ä¸­è¿æ¥æ–­å¼€: {error_type} - {error_msg_str}")
                                    logger.error("é”™è¯¯è¯Šæ–­:\n%s", diagnosis)
                                    logger.error(f"æ•°æ®æ¥æ”¶ç»Ÿè®¡: {data_chunks} ä¸ªæ•°æ®å—, {total_chars} å­—ç¬¦, è€—æ—¶ {time_elapsed:.2f} ç§’")
                                    
                                    # å¦‚æœæœ‰éƒ¨åˆ†å“åº”ï¼Œè¿”å›éƒ¨åˆ†ç»“æœï¼Œå¦åˆ™æŠ›å‡ºå¼‚å¸¸è®©é‡è¯•æœºåˆ¶å¤„ç†
                                    if not full_response:
                                        raise  # æ²¡æœ‰æ”¶åˆ°ä»»ä½•å“åº”ï¼ŒæŠ›å‡ºå¼‚å¸¸è®©é‡è¯•æœºåˆ¶å¤„ç†
                                    # æœ‰éƒ¨åˆ†å“åº”ï¼Œè¿”å›éƒ¨åˆ†ç»“æœä½†æ ‡è®°ä¸ºå¤±è´¥
                                    end_time = time.time()
                                    disconnect_type = "æœåŠ¡å™¨ä¸»åŠ¨æ–­å¼€" if is_server_disconnect else "å®¢æˆ·ç«¯è¿æ¥é”™è¯¯"
                                    return APIResponse(
                                        success=False,
                                        response_text="".join(full_response),
                                        error_msg=f"æµå¼å“åº”ä¸­æ–­ ({disconnect_type}): {error_type} - {error_msg_str}",
                                        input_tokens=prompt_tokens,
                                        output_tokens=stream_stats.total_tokens,
                                        duration=end_time - start_time,
                                        start_time=start_time,
                                        end_time=end_time,
                                        model_name=self.model,
                                        stream_stats=stream_stats,
                                        first_token_latency=first_token_latency
                                    )
                                
                                end_time = time.time()
                                return APIResponse(
                                    success=True,
                                    response_text="".join(full_response),
                                    input_tokens=prompt_tokens,
                                    output_tokens=stream_stats.total_tokens,
                                    duration=end_time - start_time,
                                    start_time=start_time,
                                    end_time=end_time,
                                    model_name=self.model,
                                    stream_stats=stream_stats,
                                    first_token_latency=first_token_latency
                                )
                            else:
                                # éæµå¼è¾“å‡ºå¤„ç†
                                data = await response.json()
                                response_text = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                                
                                # ä¼°ç®—tokenæ•°é‡
                                tokens_generated = token_counter.count_tokens(response_text, self.model)
                                
                                # æ›´æ–°æµç»Ÿè®¡ï¼ˆè™½ç„¶ä¸æ˜¯æµå¼ä½†ä»éœ€è¦è®¡ç®—é€Ÿåº¦ï¼‰
                                stream_stats.update(response_text)
                                end_time = time.time()
                                first_token_latency = end_time - start_time
                                return APIResponse(
                                    success=True,
                                    response_text=response_text,
                                    input_tokens=prompt_tokens,
                                    output_tokens=tokens_generated,
                                    duration=end_time - start_time,
                                    start_time=start_time,
                                    end_time=end_time,
                                    model_name=self.model,
                                    stream_stats=stream_stats,
                                    first_token_latency=first_token_latency
                                )
                        except Exception as e:
                            logger.error(f"æµå¼è¾“å‡ºä¸­æ–­: {e}")
                            # è¿”å›å·²ç”Ÿæˆçš„éƒ¨åˆ†å†…å®¹ï¼Œä½†æ ‡è®°ä¸ºå¤±è´¥
                            end_time = time.time()
                            response_text = ''.join(full_response)
                            return APIResponse(
                                success=False,
                                response_text=response_text,
                                error_msg=f"æµå¼è¾“å‡ºä¸­æ–­: {str(e)}",
                                input_tokens=prompt_tokens,
                                output_tokens=stream_stats.total_tokens,
                                duration=end_time - start_time,
                                start_time=start_time,
                                end_time=end_time,
                                model_name=self.model,
                                stream_stats=stream_stats
                            )
                    else:
                        error_text = await response.text()
                        logger.error(f"APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {response.status} - {error_text}")
                        logger.error(f"è¯·æ±‚URL: {self.api_url}{self.chat_path}, æ¨¡å‹: {self.model}")
                        if attempt == self.max_retries - 1:
                            return APIResponse(
                                success=False,
                                error_msg=f"HTTP {response.status}: {error_text}",
                                input_tokens=prompt_tokens,
                                duration=time.time() - start_time,
                                start_time=start_time,
                                end_time=time.time()
                            )
            
            except asyncio.TimeoutError as e:
                error_msg = "è¿æ¥è¶…æ—¶" if "connect" in str(e) else "è¯·æ±‚è¶…æ—¶"
                logger.error(f"API{error_msg} (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt == self.max_retries - 1:
                    return APIResponse(
                        success=False,
                        error_msg=error_msg,
                        input_tokens=prompt_tokens,
                        duration=time.time() - start_time,
                        start_time=start_time,
                        end_time=time.time()
                    )
            
            except asyncio.TimeoutError as e:
                # å®¢æˆ·ç«¯è¶…æ—¶ï¼šè¿æ¥æˆ–è¯»å–è¶…æ—¶
                error_msg_str = str(e)
                logger.error("APIè¿æ¥è¶…æ—¶ (å°è¯• %d/%d): %s", attempt + 1, self.max_retries, error_msg_str)
                logger.error("è¯·æ±‚URL: %s%s, æ¨¡å‹: %s", self.api_url, self.chat_path, self.model)
                logger.error("å®¢æˆ·ç«¯è¶…æ—¶ï¼šå¯èƒ½æ˜¯è¿æ¥å»ºç«‹æ…¢æˆ–æœåŠ¡å™¨å“åº”æ…¢")
                
                if attempt < self.max_retries - 1:
                    wait_time = 2 * (attempt + 1)
                    logger.info("ç­‰å¾… %d ç§’åé‡è¯•...", wait_time)
                    await asyncio.sleep(wait_time)
                    continue
                return APIResponse(
                    success=False,
                    error_msg=f"å®¢æˆ·ç«¯è¶…æ—¶: {error_msg_str}",
                    input_tokens=prompt_tokens,
                    duration=time.time() - start_time,
                    start_time=start_time,
                    end_time=time.time()
                )
            except (aiohttp.ServerDisconnectedError, aiohttp.ClientConnectionError, ConnectionError, OSError) as e:
                error_type = type(e).__name__
                error_msg_str = str(e)
                logger.error("APIè¿æ¥é”™è¯¯ (å°è¯• %d/%d): %s - %s", attempt + 1, self.max_retries, error_type, error_msg_str)
                logger.error("è¯·æ±‚URL: %s%s, æ¨¡å‹: %s", self.api_url, self.chat_path, self.model)
                
                # åŒºåˆ†æœåŠ¡å™¨ä¸»åŠ¨æ–­å¼€å’Œå®¢æˆ·ç«¯è¿æ¥é”™è¯¯
                is_server_disconnect = isinstance(e, aiohttp.ServerDisconnectedError)
                
                if is_server_disconnect:
                    logger.error("âš ï¸ æœåŠ¡å™¨ä¸»åŠ¨æ–­å¼€è¿æ¥")
                    prompt_len = len(prompt) if prompt else 0
                    logger.error("å¯èƒ½åŸå› ï¼š")
                    logger.error("  1) æœåŠ¡å™¨ç«¯è¶…æ—¶ï¼ˆè¯·æ±‚å¤„ç†æ—¶é—´è¿‡é•¿ï¼‰")
                    logger.error("  2) æœåŠ¡å™¨èµ„æºé™åˆ¶ï¼ˆå†…å­˜/GPUä¸è¶³ï¼‰")
                    logger.error("  3) æœåŠ¡å™¨è¿æ¥æ•°é™åˆ¶ï¼ˆå¹¶å‘è¿‡é«˜ï¼‰")
                    if prompt_len > 1000:
                        logger.error("  4) è¯·æ±‚è¿‡é•¿ï¼ˆ%då­—ç¬¦ï¼‰ï¼ŒæœåŠ¡å™¨å¯èƒ½æ— æ³•å¤„ç†", prompt_len)
                    logger.error("  5) æ•°æ®æ¥æ”¶é€Ÿåº¦é—®é¢˜ï¼ˆå‹åŠ›æµ‹è¯•æ—¶æ•°æ®æ¥æ”¶ä¸åŠæ—¶ï¼‰")
                else:
                    logger.error("å®¢æˆ·ç«¯è¿æ¥é”™è¯¯")
                    logger.error("å¯èƒ½åŸå› ï¼šç½‘ç»œé—®é¢˜ã€DNSè§£æå¤±è´¥ã€é˜²ç«å¢™é™åˆ¶ç­‰")
                
                # å¯¹äºè¿æ¥é”™è¯¯ï¼Œå¢åŠ é‡è¯•ç­‰å¾…æ—¶é—´ï¼Œå¹¶å°è¯•é‡æ–°åˆ›å»ºä¼šè¯
                if attempt < self.max_retries - 1:
                    # æ ¹æ®é”™è¯¯ç±»å‹å’Œè¯·æ±‚é•¿åº¦è°ƒæ•´ç­‰å¾…æ—¶é—´
                    if is_server_disconnect:
                        prompt_len = len(prompt) if prompt else 0
                        # æœåŠ¡å™¨æ–­å¼€æ—¶ï¼Œç­‰å¾…æ—¶é—´æ›´é•¿ï¼Œç‰¹åˆ«æ˜¯é•¿è¯·æ±‚
                        base_wait = 3 if prompt_len > 1000 else 2
                    else:
                        base_wait = 1
                    wait_time = base_wait * (attempt + 1)  # é€’å¢ç­‰å¾…æ—¶é—´
                    logger.info("ç­‰å¾… %d ç§’åé‡è¯•...", wait_time)
                    await asyncio.sleep(wait_time)
                    # å¦‚æœä¼šè¯å·²å…³é—­ï¼Œå°è¯•é‡æ–°åˆ›å»º
                    if self.session.closed:
                        logger.warning("æ£€æµ‹åˆ°ä¼šè¯å·²å…³é—­ï¼Œå°è¯•é‡æ–°åˆ›å»ºä¼šè¯")
                        await self._recreate_session()
                    continue  # ç»§ç»­é‡è¯•å¾ªç¯
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                disconnect_type = "æœåŠ¡å™¨ä¸»åŠ¨æ–­å¼€" if is_server_disconnect else "å®¢æˆ·ç«¯è¿æ¥é”™è¯¯"
                return APIResponse(
                    success=False,
                    error_msg=f"{disconnect_type} ({error_type}): {error_msg_str}",
                    input_tokens=prompt_tokens,
                    duration=time.time() - start_time,
                    start_time=start_time,
                    end_time=time.time()
                )
            
            except Exception as e:
                error_type = type(e).__name__
                error_msg_str = str(e)
                logger.error("APIè¯·æ±‚å¼‚å¸¸ (å°è¯• %d/%d): %s - %s", attempt + 1, self.max_retries, error_type, error_msg_str)
                logger.error("è¯·æ±‚URL: %s%s, æ¨¡å‹: %s", self.api_url, self.chat_path, self.model)
                if attempt < self.max_retries - 1:
                    wait_time = 1 * (attempt + 1)  # é€šç”¨å¼‚å¸¸ç­‰å¾…æ—¶é—´ï¼š1s, 2s, 3s...
                    logger.info("ç­‰å¾… %d ç§’åé‡è¯•...", wait_time)
                    await asyncio.sleep(wait_time)
                    continue  # ç»§ç»­é‡è¯•å¾ªç¯
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                return APIResponse(
                    success=False,
                    error_msg=f"{error_type}: {error_msg_str}",
                    input_tokens=prompt_tokens,
                    duration=time.time() - start_time,
                    start_time=start_time,
                    end_time=time.time()
                )
        
        return APIResponse(
            success=False,
            error_msg="æœªçŸ¥é”™è¯¯",
            input_tokens=prompt_tokens,
            duration=time.time() - start_time,
            start_time=start_time,
            end_time=time.time()
        )
